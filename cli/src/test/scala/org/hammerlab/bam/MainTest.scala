package org.hammerlab.bam

import java.io.ByteArrayOutputStream
import java.security.Permission

import org.hammerlab.cli.app.Cmd
import org.hammerlab.test.Suite

case class ExitException(status: Int)
  extends SecurityException("System.exit() is not allowed")

sealed class NoExitSecurityManager extends SecurityManager {
  override def checkPermission(perm: Permission): Unit = {}

  override def checkPermission(perm: Permission, context: Object): Unit = {}

  override def checkExit(status: Int): Unit = {
    super.checkExit(status)
    throw ExitException(status)
  }
}

class MainTest
  extends Suite {

  override def beforeAll(): Unit = System.setSecurityManager(new NoExitSecurityManager())

  override def afterAll(): Unit = System.setSecurityManager(null)

  def check(cmd: Cmd)(args: String*)(expected: String): Unit = {
    val stream = new ByteArrayOutputStream()
    Console.withOut(stream) {
      intercept[ExitException] {
        cmd.main.main(
          args.toArray
        )
      }
      .status should be(0)
      stream.toString should be(expected.stripMargin)
    }
  }

  def check(args: String*)(expected: String): Unit = {
    val stream = new ByteArrayOutputStream()
    Console.withOut(stream) {
      intercept[ExitException] {
        Main.main(
          args.toArray
        )
      }.status should be(0)
      stream.toString should be(expected.stripMargin)
    }
  }

  test("help") {
    check(
      "-h"
    )(
      """spark-bam 1.2.0-M1
        |Usage: spark-bam [options] [command] [command-options]
        |
        |Available commands: check-bam, check-blocks, compare-splits, compute-splits, count-reads, full-check, htsjdk-rewrite, index-blocks, index-records, time-load
        |
        |Type  spark-bam command --help  for help on an individual command
        |"""
    )
  }

  test("check help") {
    check(
      "check-bam", "-h"
    )(
      """Command: check-bam
        |Usage: spark-bam check-bam 
        |  --bgzf-blocks-to-check | -z  <num=5>
        |        When searching for BGZF-block boundaries, look this many blocks ahead to verify that a candidate is a valid block. In general, probability of a false-positive is 2^(-32N) for N blocks of look-ahead
        |  --ranges | --intervals | -i  <intervals>
        |        Comma-separated list of byte-ranges to restrict computation to; when specified, only BGZF blocks whose starts are in this set will be considered. Allowed formats: <start>-<end>, <start>+<length>, <position>. All values can take integer values or byte-size shorthands (e.g. "10m")
        |  --blocks-path | -b  <path>
        |        File with bgzf-block-start positions as output by index-blocks; If unset, the BAM path with a ".blocks" extension appended is used. If this path doesn't exist, use a parallel search for BGZF blocks (see --bgzf-block-headers-to-check)
        |  --split-size | --max-split-size | -m  <bytes>
        |        Maximum Hadoop split-size; if unset, default to underlying FileSystem's value. Integers as well as byte-size short-hands accepted, e.g. 64m, 32MB
        |  --records-path | -r  <path>
        |        File with BAM-record-start positions, as output by index-records. If unset, the BAM path with a ".records" extension appended is used
        |  --warn  <bool>
        |        Set the root logging level to WARN; useful for making Spark display the console progress-bar in client-mode
        |  --overwrite | -f  <bool>
        |        Whether to overwrite the output file, if it already exists
        |  --print-limit | -l  <num=1000000>
        |        When collecting samples of records/results for displaying to the user, limit to this many to avoid overloading the driver
        |  --results-per-partition | -p  <num=100000>
        |        After running eager and/or seqdoop checkers over a BAM file and filtering to just the contested positions, repartition to have this many records per partition. Typically there are far fewer records at this stage, so it's useful to coalesce down to avoid 1,000's of empty partitions
        |  --max-read-size  <num=10000000>
        |        Maximum number of bases long for spark-bam checkers to allow a read to be
        |  --reads-to-check  <num=10>
        |        Number of consecutive reads to check for validity before declaring a position to be read-start
        |  --spark-bam | -s  <bool>
        |        Run the spark-bam checker; if both or neither of -s and -u are set, then they are both run, and the results compared. If only one is set, its results are compared against a "ground truth" file generated by the index-records command
        |  --hadoop-bam | --upstream | -u  <bool>
        |        Run the hadoop-bam checker; if both or neither of -s and -u are set, then they are both run, and the results compared. If only one is set, its results are compared against a "ground truth" file generated by the index-records command
        |
        |"""
    )
  }

  test("htsjdk-rewrite help") {
    check(
      rewrite.HTSJDKRewrite
    )(
      "-h"
    )(
      """Rewrite BAM file with HTSJDK; records not aligned to BGZF-block boundaries
        |Usage: â€¦ org.hammerlab.bam.rewrite.Main [options]
        |  --usage  <bool>
        |        Print usage and exit
        |  --help | -h  <bool>
        |        Print help message and exit
        |  --read-ranges | -r  <ranges?>
        |  --overwrite | -f  <bool>
        |        Whether to overwrite the output file, if it already exists
        |  --index-blocks | -b  <bool>
        |  --index-records | -i  <bool>
        |
        |"""
    )
  }

  test("htsjdk-rewrite cmd help") {
    check(
      "htsjdk-rewrite", "-h"
    )(
      """Command: htsjdk-rewrite
        |Usage: spark-bam htsjdk-rewrite 
        |  --read-ranges | -r  <ranges?>
        |  --overwrite | -f  <bool>
        |        Whether to overwrite the output file, if it already exists
        |  --index-blocks | -b  <bool>
        |  --index-records | -i  <bool>
        |
        |"""
    )
  }
}
