package org.hammerlab.bam.spark.compare

import caseapp.{ AppName, ProgName, Recurse, Name â‡’ O }
import cats.Show
import cats.syntax.all._
import org.hammerlab.args.{ FindBlockArgs, FindReadArgs, IntRanges, SplitSize }
import org.hammerlab.bam.kryo._
import org.hammerlab.cli.app.Cmd
import org.hammerlab.cli.app.spark.PathApp
import org.hammerlab.cli.args.PrintLimitArgs
import org.hammerlab.hadoop.splits.MaxSplitSize
import org.hammerlab.kryo._
import org.hammerlab.paths.Path
import org.hammerlab.stats.Stats
import shapeless._

import scala.collection.mutable

object CompareSplits extends Cmd {

  @AppName("Compare splits computed from many BAM files listed in a given file")
  @ProgName("â€¦ org.hammerlab.bam.spark.compare")
  case class Opts(@Recurse printLimit: PrintLimitArgs,
                  @Recurse splitSizeArgs: SplitSize.Args,
                  @Recurse findReadArgs: FindReadArgs,
                  @Recurse findBlockArgs: FindBlockArgs,

                  @O("r")
                  fileRanges: Option[IntRanges] = None
  )

  val main = Main(
    args â‡’ new PathApp(args, Registrar) {

      val lines =
        path
          .lines
          .map(_.trim)
          .filter(_.nonEmpty)
          .zipWithIndex
          .collect {
            case (path, idx)
              if args.fileRanges.forall(_.contains(idx)) â‡’
              path
          }
          .toVector

      val numBams = lines.length

      implicit val splitSize: MaxSplitSize = opts.splitSizeArgs.maxSplitSize

      implicit val FindReadArgs(maxReadSize, readsToCheck) = opts.findReadArgs

      implicit val bgzfBlocksToCheck = opts.findBlockArgs.bgzfBlocksToCheck

      opts.splitSizeArgs.set

      val pathResults =
        new PathChecks(lines, numBams)
          .results

      import cats.implicits.{ catsKernelStdGroupForInt, catsKernelStdMonoidForVector }
      import org.hammerlab.types.Monoid._
      import shapeless._

      val (
        (timingRatios: Seq[Double]) ::  // IntelliJ needs some help on the type inference here ðŸ¤·ðŸ¼ï¸
        numSparkBamSplits ::
        numHadoopBamSplits ::
        sparkOnlySplits ::
        hadoopOnlySplits ::
        hadoopBamMS ::
        sparkBamMS ::
        HNil
      ) =
        pathResults
          .values
          .map {
            result â‡’
              // Create an HList with a Vector of timing-ratio Doubles followed by the Int fields from Result
              Vector(result.sparkBamMS.toDouble / result.hadoopBamMS) ::
                Result.gen.to(result).filter[Int]  // drop the differing-splits vector, leave+sum just the other (numeric) fields
          }
          .reduce { _ |+| _ }

      val diffs =
        pathResults
          .filter(_._2.diffs.nonEmpty)
          .collect

      import cats.Show.show
      import org.hammerlab.io.show.int

      implicit val showDouble: Show[Double] =
        show {
          "%.1f".format(_)
        }

      def printTimings(): Unit = {
        echo(
          "Total split-computation time:",
          s"\thadoop-bam:\t$hadoopBamMS",
          s"\tspark-bam:\t$sparkBamMS",
          ""
        )

        if (timingRatios.size > 1)
          echo(
            "Ratios:",
            Stats(timingRatios, onlySampleSorted = true),
            ""
          )
        else if (timingRatios.size == 1)
          echo(
            show"Ratio: ${timingRatios.head}",
            ""
          )
      }

      if (diffs.isEmpty) {
        echo(
          s"All $numBams BAMs' splits (totals: $numSparkBamSplits, $numHadoopBamSplits) matched!",
          ""
        )
        printTimings()
      } else {
        echo(
          s"${diffs.length} of $numBams BAMs' splits didn't match (totals: $numSparkBamSplits, $numHadoopBamSplits; $sparkOnlySplits, $hadoopOnlySplits unmatched)",
          ""
        )
        printTimings()
        diffs.foreach {
          case (
            path,
            Result(
              numSparkSplits,
              numHadoopSplits,
              diffs,
              numSparkOnlySplits,
              numHadoopOnlySplits,
              _,
              _
            )
          ) â‡’
            val totalsMsg =
              s"totals: $numSparkSplits, $numHadoopSplits; mismatched: $numSparkOnlySplits, $numHadoopOnlySplits"

            print(
              diffs
                .map {
                  case Left(ours) â‡’
                    show"\t$ours"
                  case Right(theirs) â‡’
                    show"\t\t$theirs"
                },
              s"\t${path.basename}: ${diffs.length} splits differ ($totalsMsg):",
              n â‡’ s"\t${path.basename}: first $n of ${diffs.length} splits that differ ($totalsMsg):"
            )
            echo("")
        }
      }
    }
  )

  case class Registrar() extends spark.Registrar(
    cls[mutable.WrappedArray.ofRef[_]],
    cls[Path],      // collected
    cls[Result],    // collected
    cls[_ :: _],    // reduced
    HNil.getClass,
    cls[Result]
  )

//  object Main extends app.Main(App)
}
